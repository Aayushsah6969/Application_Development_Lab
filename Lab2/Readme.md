# ğŸ  Simple Linear Regression - House Price Prediction

**Complete explanation** of the Simple Linear Regression experiment for predicting house sale prices using living area and number of bedrooms.

---

## ğŸ“‹ **Overview**

This lab demonstrates:
- Loading training and test datasets
- Feature selection and data cleaning
- Training a Linear Regression model
- Model evaluation using validation data
- Making predictions on unseen test data
- Creating comprehensive visualizations

**Dataset:** House Prices dataset with features like GrLivArea (above ground living area) and BedroomAbvGr (bedrooms above ground)

---

# âœ… **STEP 1 â€” Import Required Packages**

### âœ” What we did:

Imported necessary Python libraries:

* **pandas** â†’ data loading & manipulation
* **numpy** â†’ numeric operations
* **matplotlib.pyplot** â†’ plotting & visualization
* **sklearn.linear_model** â†’ LinearRegression model
* **sklearn.metrics** â†’ mean_squared_error, r2_score for evaluation
* **sklearn.model_selection** â†’ train_test_split for data splitting

### âœ” Why:

These libraries provide the essential tools for:
- Loading and processing CSV files
- Building regression models
- Evaluating model performance
- Creating visualizations

### âœ” What it shows:

Nothing visual â€” sets up the environment for machine learning workflow.

---

# âœ… **STEP 2 â€” Load Data**

### âœ” What we did:

Loaded two separate CSV files:

```python
train_df = pd.read_csv("data/train.csv")
test_df = pd.read_csv("data/test.csv")
```

### âœ” Why:

* **train.csv** contains features + target (SalePrice)
* **test.csv** contains only features (no SalePrice)

This mimics real-world scenarios where we train on labeled data and predict on unlabeled data.

### âœ” What it shows:

* First few rows of training data using `train_df.head()`
* Dataset structure with multiple columns
* Target variable: **SalePrice**

---

# âœ… **STEP 3 â€” Select Required Columns**

### âœ” What we did:

Selected specific features for modeling:

**From training data:**
* **GrLivArea** â†’ Above ground living area (square feet)
* **BedroomAbvGr** â†’ Number of bedrooms above ground
* **SalePrice** â†’ Target variable (house sale price)

**From test data:**
* **GrLivArea**
* **BedroomAbvGr**
* (No SalePrice â€” this is what we'll predict)

Removed rows with missing values using `.dropna()`.

### âœ” Why:

* Focus on relevant features that logically impact house prices
* Clean data ensures model training works properly
* Test data lacks the target variable (real prediction scenario)

### âœ” What it shows:

* `train_clean`: 3 columns (2 features + 1 target)
* `test_clean`: 2 columns (2 features only)
* All missing values removed

---

# âœ… **STEP 4 â€” Create Train/Validation Split**

### âœ” What we did:

Split the training data into two parts:

* **Training set** (80%) â†’ used to train the model
* **Validation set** (20%) â†’ used to evaluate model performance

```python
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

Where:
* **X** = features (GrLivArea, BedroomAbvGr)
* **y** = target (SalePrice)

### âœ” Why:

We need to evaluate how well the model performs on **unseen data**:

* Training on 80% of data
* Testing on remaining 20% (model has never seen this)
* This prevents overfitting and measures generalization

`random_state=42` ensures reproducibility.

### âœ” What it shows:

* X_train, y_train â†’ for model training
* X_valid, y_valid â†’ for model evaluation
* Split ratio: 80-20

---

# âœ… **STEP 5 â€” Train Model**

### âœ” What we did:

Created and trained a **Linear Regression** model:

```python
model = LinearRegression()
model.fit(X_train, y_train)
```

The model learns the equation:

```
SalePrice = intercept + (coefâ‚ Ã— GrLivArea) + (coefâ‚‚ Ã— BedroomAbvGr)
```

### âœ” Why:

Linear Regression finds the best-fit line that:
* Minimizes prediction errors
* Establishes relationship between features and target
* Provides interpretable coefficients

### âœ” What it shows:

* **Coefficients** â†’ weight of each feature
  * Positive coefficient = feature increases price
  * Negative coefficient = feature decreases price
* **Intercept** â†’ baseline price when all features are zero

Example output:
```
Coefficients: [107.13, -8247.83]
Intercept: 13456.78
```

This means:
* Each sq ft increases price by ~$107
* Each bedroom decreases price by ~$8,248 (holding area constant)

---

# âœ… **STEP 6 â€” Validate Model (Evaluate)**

### âœ” What we did:

Evaluated model performance on validation data:

```python
y_pred = model.predict(X_valid)
```

Calculated three metrics:

* **MSE (Mean Squared Error)** â†’ average of squared errors
* **RÂ² Score** â†’ proportion of variance explained (0 to 1)
* **RMSE (Root Mean Squared Error)** â†’ square root of MSE

### âœ” Why:

These metrics quantify how well the model predicts:

* **RMSE** tells us typical prediction error in dollars
* **RÂ²** shows how much variance the model captures
  * RÂ² = 0.75 means model explains 75% of price variation
  * RÂ² = 1.0 means perfect predictions

### âœ” What it shows:

Example output:
```
MSE: 1,234,567,890
RMSE: 35,136
RÂ² Score: 0.58
```

* Average prediction error: ~$35,136
* Model explains 58% of price variance
* Lower RMSE and higher RÂ² indicate better performance

---

# âœ… **STEP 7 â€” Train Final Model on Full Training Data**

### âœ” What we did:

Retrained the model on **entire training dataset** (not just 80%):

```python
final_model = LinearRegression()
final_model.fit(X, y)
```

### âœ” Why:

After validation confirms the model works well:
* Use all available training data for maximum learning
* Better final model for real predictions
* No data is "wasted" on validation anymore

### âœ” What it shows:

A refined model trained on 100% of training data, ready for deployment.

---

# âœ… **STEP 8 â€” Predict on Real Test Dataset**

### âœ” What we did:

Made predictions on the external test data:

```python
test_predictions = final_model.predict(test_clean)
```

### âœ” Why:

This is the **real application** of the model:
* Test data has no SalePrice
* Model generates price predictions
* Simulates real-world use case

### âœ” What it shows:

Array of predicted prices for each house in test dataset:
```
[254123.45, 189456.78, 312890.12, ...]
```

First 10 predictions displayed for verification.

---

# âœ… **STEP 9 â€” Save Predictions**

### âœ” What we did:

Created a CSV file with predictions:

```python
output = pd.DataFrame({
    "Id": test_df["Id"], 
    "PredictedSalePrice": test_predictions
})
output.to_csv("predictions.csv", index=False)
```

### âœ” Why:

* Save results for submission or further analysis
* Maintain house IDs for tracking
* Standardized output format

### âœ” What it shows:

**predictions.csv** file with:
* Id column â†’ house identifier
* PredictedSalePrice column â†’ model predictions

---

# ğŸ“Š **VISUALIZATION STEPS**

# âœ… **STEP 10 â€” Regression Line Visualization**

### âœ” What we did:

Plotted **GrLivArea vs SalePrice** with regression line:

* Blue scatter points â†’ actual training data
* Red line â†’ model's predicted relationship

### âœ” Why:

Visualizes how well the linear model fits the data:
* Line through the middle of points â†’ good fit
* Points far from line â†’ prediction errors
* Shows linear relationship assumption

### âœ” What it shows:

A scatter plot with regression line overlay showing the model's learned relationship between living area and price.

---

# âœ… **STEP 11 â€” Bedrooms vs SalePrice**

### âœ” What we did:

Created scatter plot of **Number of Bedrooms vs SalePrice**.

### âœ” Why:

Explore the second feature's relationship with price:
* Do more bedrooms mean higher prices?
* Is the relationship linear?
* Are there outliers?

### âœ” What it shows:

Scatter plot revealing bedroom-price relationship. Typically shows clusters at discrete bedroom counts (2, 3, 4, etc.).

---

# âœ… **STEP 12 â€” Color-Coded Scatter Plot**

### âœ” What we did:

Plotted **GrLivArea vs SalePrice** colored by **number of bedrooms**:

* X-axis: GrLivArea
* Y-axis: SalePrice
* Color: BedroomAbvGr (using viridis colormap)

### âœ” Why:

Shows **three dimensions** in a 2D plot:
* How area and price relate
* How bedrooms influence this relationship
* Identifies patterns (e.g., same area, different bedrooms = different prices)

### âœ” What it shows:

Multi-dimensional visualization with color gradient representing bedroom count. Helps understand how both features interact.

---

# âœ… **STEP 13 â€” 3D Scatter Plot**

### âœ” What we did:

Created a 3D visualization:

* X-axis: GrLivArea
* Y-axis: BedroomAbvGr
* Z-axis: SalePrice
* Color: Price (plasma colormap)

### âœ” Why:

Visualize **all three variables simultaneously**:
* See the 3D "surface" the model tries to fit
* Understand complex relationships
* Identify outliers in 3D space

### âœ” What it shows:

Interactive 3D scatter plot showing the true multidimensional nature of the data. Higher prices typically appear at higher areas.

---

# âœ… **STEP 14 â€” Residual Plot**

### âœ” What we did:

Plotted **residuals** (errors) vs predicted values:

* X-axis: Predicted SalePrice
* Y-axis: Residuals (Actual - Predicted)
* Red dashed line at y=0

### âœ” Why:

Residual analysis checks model assumptions:

* **Random scatter around zero** â†’ good model
* **Patterns or curves** â†’ model missing relationships
* **Funnel shape** â†’ heteroscedasticity (variance issues)

### âœ” What it shows:

Scatter plot of prediction errors. Ideally:
* Points randomly scattered
* Centered around zero
* No systematic patterns

---

# âœ… **STEP 15 â€” Histogram of SalePrice**

### âœ” What we did:

Plotted distribution of **SalePrice** (target variable).

### âœ” Why:

Understand target variable distribution:

* Is it normally distributed?
* Are there outliers?
* Is it skewed?

This affects model performance and assumptions.

### âœ” What it shows:

Histogram with 40 bins showing price distribution. Typically shows right-skewed distribution (few very expensive houses).

---

# ğŸ‰ **Summary (Very Helpful for Your Report)**

| Step | What We Did | Why We Did It | What It Shows |
|------|-------------|---------------|---------------|
| 1 | Imported libraries | ML tools & visualization | Setup complete |
| 2 | Loaded data | Train and test CSVs | House price dataset |
| 3 | Selected features | Focus on relevant columns | GrLivArea, BedroomAbvGr, SalePrice |
| 4 | Train/validation split | Evaluate on unseen data | 80-20 split |
| 5 | Trained model | Learn price relationships | Coefficients & intercept |
| 6 | Validated model | Measure performance | MSE, RMSE, RÂ² scores |
| 7 | Final model training | Use all training data | Refined model |
| 8 | Predicted test data | Real-world application | Price predictions |
| 9 | Saved predictions | Output results | predictions.csv |
| 10 | Regression line plot | Visualize fit | Scatter + line |
| 11 | Bedrooms scatter | Explore 2nd feature | Bedroom-price relationship |
| 12 | Color-coded scatter | Multi-dimensional view | 3 variables in 2D |
| 13 | 3D scatter plot | Full visualization | All 3 dimensions |
| 14 | Residual plot | Check model assumptions | Error distribution |
| 15 | Price histogram | Understand target | Distribution shape |

---

# ğŸ¯ **Key Outcomes**

* **Model Type:** Simple Linear Regression
* **Features Used:** 2 (GrLivArea, BedroomAbvGr)
* **Target Variable:** SalePrice
* **Dataset:** House Prices
  * Training: ~1,460 samples
  * Test: ~1,459 samples
* **Model Equation:** 
  ```
  SalePrice = intercept + Î²â‚Ã—GrLivArea + Î²â‚‚Ã—BedroomAbvGr
  ```
* **Evaluation Metrics:** MSE, RMSE, RÂ²
* **Output:** predictions.csv with predicted prices for test data

---

# ğŸ“ **Files in This Lab**

* **experiment.ipynb** - Complete regression workflow notebook
* **data/train.csv** - Training dataset (with SalePrice)
* **data/test.csv** - Test dataset (without SalePrice)
* **predictions.csv** - Model predictions output
* **requirements.txt** - Python dependencies
* **Plots/** - Saved visualization images
* **experiment.html** - HTML export of notebook

---

# ğŸ’¡ **Key Insights**

1. **Living Area Impact:** Larger homes generally sell for higher prices (positive coefficient)
2. **Bedroom Paradox:** More bedrooms can decrease price when holding area constant (suggests smaller room sizes)
3. **Model Performance:** RÂ² score indicates how much variance the model explains
4. **Residual Analysis:** Random scatter confirms linear assumptions are reasonable
5. **Price Distribution:** Typically right-skewed with high-value outliers

---

# ğŸš€ **Difference from Multiple Linear Regression (Lab 3)**

| Aspect | Lab 2 (Simple) | Lab 3 (Multiple) |
|--------|----------------|------------------|
| Features | 2 features | 11 features |
| Model Complexity | Simple relationship | Complex interactions |
| Use Case | Basic prediction | Comprehensive analysis |
| Dataset | House prices | Exam scores |

---
